{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Import various modules for string cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Read data from files \n",
    "train = pd.read_csv( \"labeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "test = pd.read_csv( \"testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\n",
    "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
    " delimiter=\"\\t\", quoting=3 )\n",
    "\n",
    "# Verify the number of reviews that were read (100,000 in total)\n",
    "print(\"Read %d labeled train reviews, %d labeled test reviews, \" \\\n",
    " \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n",
    " test[\"review\"].size, unlabeled_train[\"review\"].size ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist( review, remove_stopwords=False ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    #  \n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
    "    #\n",
    "    # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "    #\n",
    "    # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 0. Remove URL from reviews\n",
    "    no_url = re.sub(\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\",\" \", review)\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(no_url.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertomariopirovano/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from unlabeled set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertomariopirovano/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "\n",
    "print(\"Parsing sentences from unlabeled set\")\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795532\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maybe', 'i', 'just', 'want', 'to', 'get', 'a', 'certain', 'insight', 'into', 'this', 'guy', 'who', 'i', 'thought', 'was', 'really', 'cool', 'in', 'the', 'eighties', 'just', 'to', 'maybe', 'make', 'up', 'my', 'mind', 'whether', 'he', 'is', 'guilty', 'or', 'innocent']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-11 10:46:29,862 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-11 10:46:29,905 : INFO : collecting all words and their counts\n",
      "2017-03-11 10:46:29,906 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-03-11 10:46:29,955 : INFO : PROGRESS: at sentence #10000, processed 225799 words, keeping 17773 word types\n",
      "2017-03-11 10:46:30,007 : INFO : PROGRESS: at sentence #20000, processed 451864 words, keeping 24940 word types\n",
      "2017-03-11 10:46:30,057 : INFO : PROGRESS: at sentence #30000, processed 671284 words, keeping 30022 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-11 10:46:30,109 : INFO : PROGRESS: at sentence #40000, processed 897753 words, keeping 34337 word types\n",
      "2017-03-11 10:46:30,159 : INFO : PROGRESS: at sentence #50000, processed 1116926 words, keeping 37748 word types\n",
      "2017-03-11 10:46:30,210 : INFO : PROGRESS: at sentence #60000, processed 1338325 words, keeping 40707 word types\n",
      "2017-03-11 10:46:30,260 : INFO : PROGRESS: at sentence #70000, processed 1561492 words, keeping 43317 word types\n",
      "2017-03-11 10:46:30,314 : INFO : PROGRESS: at sentence #80000, processed 1780792 words, keeping 45695 word types\n",
      "2017-03-11 10:46:30,365 : INFO : PROGRESS: at sentence #90000, processed 2004860 words, keeping 48112 word types\n",
      "2017-03-11 10:46:30,417 : INFO : PROGRESS: at sentence #100000, processed 2226848 words, keeping 50183 word types\n",
      "2017-03-11 10:46:30,468 : INFO : PROGRESS: at sentence #110000, processed 2446430 words, keeping 52056 word types\n",
      "2017-03-11 10:46:30,519 : INFO : PROGRESS: at sentence #120000, processed 2668576 words, keeping 54090 word types\n",
      "2017-03-11 10:46:30,574 : INFO : PROGRESS: at sentence #130000, processed 2894109 words, keeping 55818 word types\n",
      "2017-03-11 10:46:30,627 : INFO : PROGRESS: at sentence #140000, processed 3106772 words, keeping 57315 word types\n",
      "2017-03-11 10:46:30,684 : INFO : PROGRESS: at sentence #150000, processed 3332396 words, keeping 59023 word types\n",
      "2017-03-11 10:46:30,736 : INFO : PROGRESS: at sentence #160000, processed 3555092 words, keeping 60586 word types\n",
      "2017-03-11 10:46:30,791 : INFO : PROGRESS: at sentence #170000, processed 3778420 words, keeping 62044 word types\n",
      "2017-03-11 10:46:30,845 : INFO : PROGRESS: at sentence #180000, processed 3998942 words, keeping 63457 word types\n",
      "2017-03-11 10:46:30,899 : INFO : PROGRESS: at sentence #190000, processed 4224149 words, keeping 64755 word types\n",
      "2017-03-11 10:46:30,952 : INFO : PROGRESS: at sentence #200000, processed 4448271 words, keeping 66044 word types\n",
      "2017-03-11 10:46:31,005 : INFO : PROGRESS: at sentence #210000, processed 4669675 words, keeping 67343 word types\n",
      "2017-03-11 10:46:31,058 : INFO : PROGRESS: at sentence #220000, processed 4894605 words, keeping 68651 word types\n",
      "2017-03-11 10:46:31,111 : INFO : PROGRESS: at sentence #230000, processed 5117183 words, keeping 69911 word types\n",
      "2017-03-11 10:46:31,166 : INFO : PROGRESS: at sentence #240000, processed 5344663 words, keeping 71118 word types\n",
      "2017-03-11 10:46:31,217 : INFO : PROGRESS: at sentence #250000, processed 5558799 words, keeping 72302 word types\n",
      "2017-03-11 10:46:31,271 : INFO : PROGRESS: at sentence #260000, processed 5778744 words, keeping 73428 word types\n",
      "2017-03-11 10:46:31,325 : INFO : PROGRESS: at sentence #270000, processed 6000047 words, keeping 74716 word types\n",
      "2017-03-11 10:46:31,383 : INFO : PROGRESS: at sentence #280000, processed 6225912 words, keeping 76318 word types\n",
      "2017-03-11 10:46:31,439 : INFO : PROGRESS: at sentence #290000, processed 6449075 words, keeping 77786 word types\n",
      "2017-03-11 10:46:31,496 : INFO : PROGRESS: at sentence #300000, processed 6673650 words, keeping 79115 word types\n",
      "2017-03-11 10:46:31,548 : INFO : PROGRESS: at sentence #310000, processed 6899007 words, keeping 80424 word types\n",
      "2017-03-11 10:46:31,602 : INFO : PROGRESS: at sentence #320000, processed 7123833 words, keeping 81752 word types\n",
      "2017-03-11 10:46:31,655 : INFO : PROGRESS: at sentence #330000, processed 7345644 words, keeping 82974 word types\n",
      "2017-03-11 10:46:31,709 : INFO : PROGRESS: at sentence #340000, processed 7575120 words, keeping 84219 word types\n",
      "2017-03-11 10:46:31,766 : INFO : PROGRESS: at sentence #350000, processed 7798318 words, keeping 85364 word types\n",
      "2017-03-11 10:46:31,821 : INFO : PROGRESS: at sentence #360000, processed 8018937 words, keeping 86533 word types\n",
      "2017-03-11 10:46:31,885 : INFO : PROGRESS: at sentence #370000, processed 8246110 words, keeping 87645 word types\n",
      "2017-03-11 10:46:31,942 : INFO : PROGRESS: at sentence #380000, processed 8471273 words, keeping 88813 word types\n",
      "2017-03-11 10:46:31,998 : INFO : PROGRESS: at sentence #390000, processed 8700976 words, keeping 89840 word types\n",
      "2017-03-11 10:46:32,052 : INFO : PROGRESS: at sentence #400000, processed 8923843 words, keeping 90849 word types\n",
      "2017-03-11 10:46:32,111 : INFO : PROGRESS: at sentence #410000, processed 9145229 words, keeping 91812 word types\n",
      "2017-03-11 10:46:32,166 : INFO : PROGRESS: at sentence #420000, processed 9366273 words, keeping 92843 word types\n",
      "2017-03-11 10:46:32,221 : INFO : PROGRESS: at sentence #430000, processed 9593905 words, keeping 93862 word types\n",
      "2017-03-11 10:46:32,276 : INFO : PROGRESS: at sentence #440000, processed 9820535 words, keeping 94834 word types\n",
      "2017-03-11 10:46:32,331 : INFO : PROGRESS: at sentence #450000, processed 10044321 words, keeping 95963 word types\n",
      "2017-03-11 10:46:32,388 : INFO : PROGRESS: at sentence #460000, processed 10277031 words, keeping 97015 word types\n",
      "2017-03-11 10:46:32,443 : INFO : PROGRESS: at sentence #470000, processed 10504921 words, keeping 97857 word types\n",
      "2017-03-11 10:46:32,496 : INFO : PROGRESS: at sentence #480000, processed 10725393 words, keeping 98787 word types\n",
      "2017-03-11 10:46:32,551 : INFO : PROGRESS: at sentence #490000, processed 10952079 words, keeping 99791 word types\n",
      "2017-03-11 10:46:32,605 : INFO : PROGRESS: at sentence #500000, processed 11173619 words, keeping 100682 word types\n",
      "2017-03-11 10:46:32,659 : INFO : PROGRESS: at sentence #510000, processed 11398932 words, keeping 101615 word types\n",
      "2017-03-11 10:46:32,714 : INFO : PROGRESS: at sentence #520000, processed 11622243 words, keeping 102514 word types\n",
      "2017-03-11 10:46:32,767 : INFO : PROGRESS: at sentence #530000, processed 11846721 words, keeping 103315 word types\n",
      "2017-03-11 10:46:32,824 : INFO : PROGRESS: at sentence #540000, processed 12071314 words, keeping 104175 word types\n",
      "2017-03-11 10:46:32,880 : INFO : PROGRESS: at sentence #550000, processed 12296748 words, keeping 105041 word types\n",
      "2017-03-11 10:46:32,934 : INFO : PROGRESS: at sentence #560000, processed 12517997 words, keeping 105904 word types\n",
      "2017-03-11 10:46:32,990 : INFO : PROGRESS: at sentence #570000, processed 12747019 words, keeping 106692 word types\n",
      "2017-03-11 10:46:33,044 : INFO : PROGRESS: at sentence #580000, processed 12968536 words, keeping 107568 word types\n",
      "2017-03-11 10:46:33,101 : INFO : PROGRESS: at sentence #590000, processed 13194027 words, keeping 108404 word types\n",
      "2017-03-11 10:46:33,158 : INFO : PROGRESS: at sentence #600000, processed 13416215 words, keeping 109120 word types\n",
      "2017-03-11 10:46:33,212 : INFO : PROGRESS: at sentence #610000, processed 13637250 words, keeping 109992 word types\n",
      "2017-03-11 10:46:33,267 : INFO : PROGRESS: at sentence #620000, processed 13863561 words, keeping 110735 word types\n",
      "2017-03-11 10:46:33,323 : INFO : PROGRESS: at sentence #630000, processed 14087953 words, keeping 111507 word types\n",
      "2017-03-11 10:46:33,381 : INFO : PROGRESS: at sentence #640000, processed 14308584 words, keeping 112311 word types\n",
      "2017-03-11 10:46:33,438 : INFO : PROGRESS: at sentence #650000, processed 14534291 words, keeping 113090 word types\n",
      "2017-03-11 10:46:33,494 : INFO : PROGRESS: at sentence #660000, processed 14757044 words, keeping 113839 word types\n",
      "2017-03-11 10:46:33,553 : INFO : PROGRESS: at sentence #670000, processed 14980449 words, keeping 114535 word types\n",
      "2017-03-11 10:46:33,610 : INFO : PROGRESS: at sentence #680000, processed 15205411 words, keeping 115246 word types\n",
      "2017-03-11 10:46:33,667 : INFO : PROGRESS: at sentence #690000, processed 15427489 words, keeping 116023 word types\n",
      "2017-03-11 10:46:33,728 : INFO : PROGRESS: at sentence #700000, processed 15656150 words, keeping 116835 word types\n",
      "2017-03-11 10:46:33,785 : INFO : PROGRESS: at sentence #710000, processed 15879145 words, keeping 117487 word types\n",
      "2017-03-11 10:46:33,842 : INFO : PROGRESS: at sentence #720000, processed 16104423 words, keeping 118110 word types\n",
      "2017-03-11 10:46:33,898 : INFO : PROGRESS: at sentence #730000, processed 16330816 words, keeping 118840 word types\n",
      "2017-03-11 10:46:33,959 : INFO : PROGRESS: at sentence #740000, processed 16551809 words, keeping 119549 word types\n",
      "2017-03-11 10:46:34,013 : INFO : PROGRESS: at sentence #750000, processed 16770129 words, keeping 120173 word types\n",
      "2017-03-11 10:46:34,069 : INFO : PROGRESS: at sentence #760000, processed 16989480 words, keeping 120807 word types\n",
      "2017-03-11 10:46:34,127 : INFO : PROGRESS: at sentence #770000, processed 17216595 words, keeping 121580 word types\n",
      "2017-03-11 10:46:34,186 : INFO : PROGRESS: at sentence #780000, processed 17446745 words, keeping 122277 word types\n",
      "2017-03-11 10:46:34,242 : INFO : PROGRESS: at sentence #790000, processed 17673841 words, keeping 122940 word types\n",
      "2017-03-11 10:46:34,274 : INFO : collected 123377 word types from a corpus of 17796812 raw words and 795532 sentences\n",
      "2017-03-11 10:46:34,275 : INFO : Loading a fresh vocabulary\n",
      "2017-03-11 10:46:34,387 : INFO : min_count=40 retains 16485 unique words (13% of original 123377, drops 106892)\n",
      "2017-03-11 10:46:34,388 : INFO : min_count=40 leaves 17237934 word corpus (96% of original 17796812, drops 558878)\n",
      "2017-03-11 10:46:34,464 : INFO : deleting the raw counts dictionary of 123377 items\n",
      "2017-03-11 10:46:34,468 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2017-03-11 10:46:34,470 : INFO : downsampling leaves estimated 12748565 word corpus (74.0% of prior 17237934)\n",
      "2017-03-11 10:46:34,471 : INFO : estimated required memory for 16485 words and 300 dimensions: 47806500 bytes\n",
      "2017-03-11 10:46:34,571 : INFO : resetting layer weights\n",
      "2017-03-11 10:46:34,980 : INFO : training model with 4 workers on 16485 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2017-03-11 10:46:34,981 : INFO : expecting 795532 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-03-11 10:46:36,033 : INFO : PROGRESS: at 0.79% examples, 494690 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:37,035 : INFO : PROGRESS: at 1.67% examples, 525233 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:38,036 : INFO : PROGRESS: at 2.55% examples, 536123 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:39,048 : INFO : PROGRESS: at 3.46% examples, 543599 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:40,062 : INFO : PROGRESS: at 4.33% examples, 543365 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:41,081 : INFO : PROGRESS: at 5.19% examples, 542948 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:46:42,082 : INFO : PROGRESS: at 6.05% examples, 543887 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:43,086 : INFO : PROGRESS: at 6.64% examples, 521323 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:46:44,124 : INFO : PROGRESS: at 7.31% examples, 508991 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:45,126 : INFO : PROGRESS: at 8.13% examples, 510138 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:46:46,126 : INFO : PROGRESS: at 8.94% examples, 510579 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:47,150 : INFO : PROGRESS: at 9.72% examples, 509332 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:46:48,159 : INFO : PROGRESS: at 10.55% examples, 509948 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:49,174 : INFO : PROGRESS: at 11.37% examples, 510742 words/s, in_qsize 5, out_qsize 2\n",
      "2017-03-11 10:46:50,181 : INFO : PROGRESS: at 12.19% examples, 512169 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:51,192 : INFO : PROGRESS: at 12.97% examples, 510671 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:46:52,197 : INFO : PROGRESS: at 13.78% examples, 510760 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:53,203 : INFO : PROGRESS: at 14.59% examples, 511204 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:54,213 : INFO : PROGRESS: at 15.44% examples, 512635 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:46:55,226 : INFO : PROGRESS: at 16.27% examples, 513119 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:56,231 : INFO : PROGRESS: at 17.08% examples, 513085 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:46:57,253 : INFO : PROGRESS: at 17.93% examples, 513944 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:58,256 : INFO : PROGRESS: at 18.73% examples, 513608 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:46:59,266 : INFO : PROGRESS: at 19.56% examples, 514040 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:47:00,282 : INFO : PROGRESS: at 20.35% examples, 513457 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:01,307 : INFO : PROGRESS: at 21.16% examples, 513292 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:47:02,330 : INFO : PROGRESS: at 22.02% examples, 513701 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:03,354 : INFO : PROGRESS: at 22.88% examples, 514321 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:04,371 : INFO : PROGRESS: at 23.74% examples, 515025 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:05,374 : INFO : PROGRESS: at 24.59% examples, 515681 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:06,383 : INFO : PROGRESS: at 25.39% examples, 515467 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:07,390 : INFO : PROGRESS: at 26.22% examples, 515560 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:08,394 : INFO : PROGRESS: at 27.03% examples, 515449 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:09,400 : INFO : PROGRESS: at 27.86% examples, 515778 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:10,410 : INFO : PROGRESS: at 28.69% examples, 516201 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:11,419 : INFO : PROGRESS: at 29.50% examples, 516028 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:47:12,436 : INFO : PROGRESS: at 30.35% examples, 516537 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:13,442 : INFO : PROGRESS: at 31.16% examples, 516409 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:14,442 : INFO : PROGRESS: at 31.99% examples, 517069 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:15,446 : INFO : PROGRESS: at 32.82% examples, 517332 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:16,449 : INFO : PROGRESS: at 33.64% examples, 517417 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:17,458 : INFO : PROGRESS: at 34.44% examples, 517257 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:47:18,478 : INFO : PROGRESS: at 35.29% examples, 517458 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:19,493 : INFO : PROGRESS: at 36.14% examples, 517870 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:20,510 : INFO : PROGRESS: at 36.97% examples, 517934 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:21,515 : INFO : PROGRESS: at 37.82% examples, 518427 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:22,531 : INFO : PROGRESS: at 38.67% examples, 518774 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:23,531 : INFO : PROGRESS: at 39.50% examples, 518981 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:24,534 : INFO : PROGRESS: at 40.33% examples, 519296 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:25,553 : INFO : PROGRESS: at 41.17% examples, 519441 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:26,568 : INFO : PROGRESS: at 42.04% examples, 519755 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:27,578 : INFO : PROGRESS: at 42.90% examples, 520120 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:28,618 : INFO : PROGRESS: at 43.78% examples, 520437 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:29,629 : INFO : PROGRESS: at 44.59% examples, 520103 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:30,640 : INFO : PROGRESS: at 45.38% examples, 519762 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:31,661 : INFO : PROGRESS: at 46.21% examples, 519605 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:32,675 : INFO : PROGRESS: at 47.06% examples, 519749 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:33,675 : INFO : PROGRESS: at 47.80% examples, 519058 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:34,689 : INFO : PROGRESS: at 48.45% examples, 517191 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:35,699 : INFO : PROGRESS: at 49.28% examples, 517291 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:47:36,699 : INFO : PROGRESS: at 50.11% examples, 517599 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:37,703 : INFO : PROGRESS: at 50.92% examples, 517518 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:38,713 : INFO : PROGRESS: at 51.77% examples, 517949 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:39,714 : INFO : PROGRESS: at 52.49% examples, 517017 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:40,719 : INFO : PROGRESS: at 53.18% examples, 515859 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:41,720 : INFO : PROGRESS: at 53.98% examples, 515721 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:42,747 : INFO : PROGRESS: at 54.81% examples, 515821 words/s, in_qsize 7, out_qsize 1\n",
      "2017-03-11 10:47:43,770 : INFO : PROGRESS: at 55.63% examples, 515747 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:44,775 : INFO : PROGRESS: at 56.47% examples, 515902 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:45,782 : INFO : PROGRESS: at 57.37% examples, 516737 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:46,782 : INFO : PROGRESS: at 58.25% examples, 517409 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:47,783 : INFO : PROGRESS: at 59.10% examples, 517564 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:48,796 : INFO : PROGRESS: at 59.98% examples, 518210 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:49,800 : INFO : PROGRESS: at 60.85% examples, 518713 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:50,809 : INFO : PROGRESS: at 61.77% examples, 519437 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-11 10:47:51,817 : INFO : PROGRESS: at 62.69% examples, 520173 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:52,832 : INFO : PROGRESS: at 63.63% examples, 521011 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:47:53,833 : INFO : PROGRESS: at 64.52% examples, 521565 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:54,838 : INFO : PROGRESS: at 65.44% examples, 522336 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:55,861 : INFO : PROGRESS: at 66.42% examples, 523325 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:56,873 : INFO : PROGRESS: at 67.31% examples, 523845 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:47:57,883 : INFO : PROGRESS: at 68.16% examples, 524016 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:58,888 : INFO : PROGRESS: at 69.01% examples, 524221 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:47:59,889 : INFO : PROGRESS: at 69.89% examples, 524687 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:00,892 : INFO : PROGRESS: at 70.73% examples, 524800 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:48:01,905 : INFO : PROGRESS: at 71.56% examples, 524842 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:02,921 : INFO : PROGRESS: at 72.28% examples, 524062 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:48:03,928 : INFO : PROGRESS: at 73.16% examples, 524405 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:48:04,938 : INFO : PROGRESS: at 74.02% examples, 524642 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:05,940 : INFO : PROGRESS: at 74.93% examples, 525233 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:06,947 : INFO : PROGRESS: at 75.82% examples, 525706 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:07,956 : INFO : PROGRESS: at 76.71% examples, 526075 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:08,969 : INFO : PROGRESS: at 77.57% examples, 526267 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:09,969 : INFO : PROGRESS: at 78.41% examples, 526370 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:10,976 : INFO : PROGRESS: at 79.29% examples, 526662 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:11,990 : INFO : PROGRESS: at 80.17% examples, 526983 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:13,004 : INFO : PROGRESS: at 81.06% examples, 527366 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:14,023 : INFO : PROGRESS: at 82.00% examples, 527857 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:15,033 : INFO : PROGRESS: at 82.88% examples, 528116 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:16,037 : INFO : PROGRESS: at 83.82% examples, 528736 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:17,043 : INFO : PROGRESS: at 84.74% examples, 529278 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:18,044 : INFO : PROGRESS: at 85.65% examples, 529752 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:48:19,048 : INFO : PROGRESS: at 86.53% examples, 529932 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:20,052 : INFO : PROGRESS: at 87.40% examples, 530178 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:21,057 : INFO : PROGRESS: at 88.29% examples, 530477 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:48:22,064 : INFO : PROGRESS: at 89.22% examples, 531101 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:23,064 : INFO : PROGRESS: at 90.15% examples, 531681 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:24,078 : INFO : PROGRESS: at 91.05% examples, 532046 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:25,084 : INFO : PROGRESS: at 91.95% examples, 532441 words/s, in_qsize 6, out_qsize 1\n",
      "2017-03-11 10:48:26,090 : INFO : PROGRESS: at 92.90% examples, 533081 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:27,104 : INFO : PROGRESS: at 93.84% examples, 533622 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:48:28,111 : INFO : PROGRESS: at 94.77% examples, 534120 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:29,147 : INFO : PROGRESS: at 95.67% examples, 534347 words/s, in_qsize 8, out_qsize 1\n",
      "2017-03-11 10:48:30,161 : INFO : PROGRESS: at 96.59% examples, 534672 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:31,171 : INFO : PROGRESS: at 97.50% examples, 535011 words/s, in_qsize 7, out_qsize 0\n",
      "2017-03-11 10:48:32,181 : INFO : PROGRESS: at 98.42% examples, 535470 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:48:33,194 : INFO : PROGRESS: at 99.29% examples, 535537 words/s, in_qsize 8, out_qsize 0\n",
      "2017-03-11 10:48:33,997 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-03-11 10:48:34,013 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-03-11 10:48:34,025 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-03-11 10:48:34,032 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-03-11 10:48:34,033 : INFO : training on 88984060 raw words (63744151 effective words) took 119.0s, 535601 effective words/s\n",
      "2017-03-11 10:48:34,034 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-03-11 10:48:34,238 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-03-11 10:48:34,239 : INFO : not storing attribute syn0norm\n",
      "2017-03-11 10:48:34,240 : INFO : not storing attribute cum_table\n",
      "2017-03-11 10:48:35,513 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'god'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"desk table chair god\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'movie'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"phone mobile movie network\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"france england germany berlin\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'austria'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"paris berlin london austria\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.6254433393478394),\n",
       " ('lady', 0.5991166234016418),\n",
       " ('millionaire', 0.5377988219261169),\n",
       " ('lad', 0.5343524813652039),\n",
       " ('chap', 0.5198793411254883),\n",
       " ('guy', 0.5184804201126099),\n",
       " ('farmer', 0.5150920152664185),\n",
       " ('person', 0.5096235871315002),\n",
       " ('monk', 0.4978397488594055),\n",
       " ('men', 0.49767956137657166)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('princess', 0.6834498047828674),\n",
       " ('belle', 0.6295305490493774),\n",
       " ('latifah', 0.6238353252410889),\n",
       " ('bride', 0.6002123355865479),\n",
       " ('nun', 0.5854156613349915),\n",
       " ('seductress', 0.5845000147819519),\n",
       " ('regina', 0.5790630578994751),\n",
       " ('stepmother', 0.5752915143966675),\n",
       " ('prince', 0.5752872228622437),\n",
       " ('goddess', 0.5739991664886475)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('goodness', 0.6461307406425476),\n",
       " ('jesus', 0.5351325273513794),\n",
       " ('heavens', 0.5024672150611877),\n",
       " ('holy', 0.49229785799980164),\n",
       " ('gosh', 0.48883056640625),\n",
       " ('heaven', 0.47777146100997925),\n",
       " ('gods', 0.46265023946762085),\n",
       " ('dear', 0.4312209188938141),\n",
       " ('grail', 0.4188128113746643),\n",
       " ('wrath', 0.4008309543132782)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"god\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'Queen' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-cd62f85946b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Queen\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/albertomariopirovano/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/albertomariopirovano/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'Queen' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.most_similar(\"Queen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.7763335704803467),\n",
       " ('horrible', 0.7417645454406738),\n",
       " ('abysmal', 0.7341878414154053),\n",
       " ('dreadful', 0.7324559092521667),\n",
       " ('atrocious', 0.7271116971969604),\n",
       " ('horrendous', 0.7015389204025269),\n",
       " ('appalling', 0.6938928961753845),\n",
       " ('horrid', 0.673977255821228),\n",
       " ('lousy', 0.6452018618583679),\n",
       " ('laughable', 0.6174856424331665)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mansion', 0.741044282913208),\n",
       " ('farmhouse', 0.6267083287239075),\n",
       " ('apartment', 0.6173556447029114),\n",
       " ('cabin', 0.6168539524078369),\n",
       " ('basement', 0.5831036567687988),\n",
       " ('castle', 0.5814731121063232),\n",
       " ('hotel', 0.5772657990455627),\n",
       " ('houses', 0.5624116659164429),\n",
       " ('cemetery', 0.5523293614387512),\n",
       " ('room', 0.5395114421844482)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('desk', 0.6891450881958008),\n",
       " ('floor', 0.6831225156784058),\n",
       " ('couch', 0.6608123779296875),\n",
       " ('balcony', 0.6509571075439453),\n",
       " ('ceiling', 0.6409872770309448),\n",
       " ('coffee', 0.6386309862136841),\n",
       " ('roof', 0.6307677626609802),\n",
       " ('diner', 0.6298085451126099),\n",
       " ('wheel', 0.6163753271102905),\n",
       " ('lawn', 0.6120452880859375)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computers', 0.6148785352706909),\n",
       " ('technology', 0.5999691486358643),\n",
       " ('generated', 0.5970050692558289),\n",
       " ('cgi', 0.586999773979187),\n",
       " ('digital', 0.5721021890640259),\n",
       " ('software', 0.5582946538925171),\n",
       " ('graphics', 0.526003360748291),\n",
       " ('equipment', 0.5192840695381165),\n",
       " ('cg', 0.5190702080726624),\n",
       " ('laser', 0.5137609839439392)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cell', 0.6229346990585327),\n",
       " ('fires', 0.619251549243927),\n",
       " ('trucks', 0.6119228601455688),\n",
       " ('washing', 0.608405590057373),\n",
       " ('tanks', 0.598710834980011),\n",
       " ('laser', 0.5966479182243347),\n",
       " ('furniture', 0.5891460180282593),\n",
       " ('motor', 0.5792536735534668),\n",
       " ('phones', 0.5780150890350342),\n",
       " ('parachute', 0.5776175260543823)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"mobile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('truck', 0.7519577741622925),\n",
       " ('jeep', 0.6660259962081909),\n",
       " ('bus', 0.665249228477478),\n",
       " ('bike', 0.6515318155288696),\n",
       " ('plane', 0.6242357492446899),\n",
       " ('garage', 0.6217759847640991),\n",
       " ('helicopter', 0.6137675046920776),\n",
       " ('boat', 0.6016305685043335),\n",
       " ('cab', 0.5965676307678223),\n",
       " ('train', 0.5895345211029053)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cigar', 0.549802839756012),\n",
       " ('tin', 0.5442754030227661),\n",
       " ('butter', 0.542699933052063),\n",
       " ('balls', 0.5180373191833496),\n",
       " ('pee', 0.5163849592208862),\n",
       " ('duck', 0.5100090503692627),\n",
       " ('juice', 0.49665337800979614),\n",
       " ('sweat', 0.4954224228858948),\n",
       " ('chips', 0.4887160360813141),\n",
       " ('gum', 0.4798174500465393)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"dice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'digit' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-85e5dbb69b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"digit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/albertomariopirovano/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/albertomariopirovano/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot compute similarity with no input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'digit' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.most_similar(\"digit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abundance', 0.6164333820343018),\n",
       " ('oddity', 0.6106451749801636),\n",
       " ('alarming', 0.610059380531311),\n",
       " ('afterthought', 0.6100172996520996),\n",
       " ('abomination', 0.5960849523544312),\n",
       " ('embarrassment', 0.592820405960083),\n",
       " ('ounce', 0.5902347564697266),\n",
       " ('excess', 0.5827727317810059),\n",
       " ('insult', 0.5797513723373413),\n",
       " ('observer', 0.5775488615036011)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"exercise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('decent', 0.6829209327697754),\n",
       " ('bad', 0.6340751051902771),\n",
       " ('great', 0.6212310791015625),\n",
       " ('nice', 0.6000409126281738),\n",
       " ('fine', 0.5729934573173523),\n",
       " ('cool', 0.5637296438217163),\n",
       " ('mediocre', 0.5629239678382874),\n",
       " ('passable', 0.5612195730209351),\n",
       " ('lousy', 0.5608699321746826),\n",
       " ('solid', 0.527820885181427)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fur', 0.7448852062225342),\n",
       " ('tinted', 0.7302852869033813),\n",
       " ('helmet', 0.6969269514083862),\n",
       " ('blue', 0.6964552998542786),\n",
       " ('colored', 0.6921685934066772),\n",
       " ('red', 0.692031741142273),\n",
       " ('trench', 0.6850919723510742),\n",
       " ('horns', 0.6850051283836365),\n",
       " ('leather', 0.6819203495979309),\n",
       " ('masks', 0.6747287511825562)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sun', 0.680997371673584),\n",
       " ('skies', 0.6526676416397095),\n",
       " ('clouds', 0.6435355544090271),\n",
       " ('river', 0.6240108013153076),\n",
       " ('heat', 0.6215382814407349),\n",
       " ('rain', 0.6108661890029907),\n",
       " ('sand', 0.6089564561843872),\n",
       " ('bridge', 0.6045998930931091),\n",
       " ('mist', 0.597465455532074),\n",
       " ('water', 0.597454309463501)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"sky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-11 10:52:53,439 : WARNING : direct access to syn0 will not be supported in future gensim releases, please use model.wv.syn0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-11 10:53:08,415 : WARNING : direct access to syn0 will not be supported in future gensim releases, please use model.wv.syn0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16485, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model[\"flower\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
