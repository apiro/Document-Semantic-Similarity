{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import gensim\n",
    "import re\n",
    "import unicodedata\n",
    "import datetime\n",
    "import collections\n",
    "import random\n",
    "import time\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Phrases\n",
    "import treetaggerwrapper\n",
    "import string\n",
    "import os\n",
    "import operator\n",
    "import platform\n",
    "from sklearn.manifold import TSNE\n",
    "treetaggerPath = str(os.getcwd()) + \"/treetagger/\" + str(platform.system()) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/albertomariopirovano/Documents/Programming/python/notebooks/tis_notebooks/treetagger/Darwin/'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treetaggerPath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trains24 = pd.read_csv(\"./Dataset/articoliS24O.csv\", delimiter=\"\\t\")\n",
    "trainRadiocor = pd.read_csv(\"./Dataset/articoliRadiocor.csv\", delimiter='\\t')\n",
    "\n",
    "colsel_trains24 = trains24[['identificativo', 'body']]\n",
    "\n",
    "colsel_trainRadiocor = trainRadiocor[['identificativo', 'body']]\n",
    "\n",
    "colsel_merged = pd.concat([colsel_trains24, colsel_trainRadiocor])\n",
    "\n",
    "colsel_merged.dropna(inplace=True)\n",
    "colsel_merged = colsel_merged.reset_index()\n",
    "colsel_merged = colsel_merged.drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAPE BEFORE REMOVING DUPLICATES: (10150, 2)\n",
      "SHAPE AFTER REMOVING DUPLICATES: (9283, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"SHAPE BEFORE REMOVING DUPLICATES: \" + str(colsel_merged.shape)) \n",
    "colsel_merged.drop_duplicates(inplace=True) \n",
    "print(\"SHAPE AFTER REMOVING DUPLICATES: \" + str(colsel_merged.shape))\n",
    "\n",
    "colsel_merged['body'] = colsel_merged['body'].apply(lambda x : str(x).lower())\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r'(\\\\)+[a-z]\\{[0-9]+\\}','')\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace('\\n','')\n",
    "\n",
    "#how to deal with times?\n",
    "#colsel_merged['body'] = colsel_merged['body'].str.replace('[0-9]+\\.[0-9]+','')\n",
    "\n",
    "#handle percentages\n",
    "colsel_merged['body'] = colsel_merged['body'].apply(lambda x: str(re.sub(r'([0-9]+)((\\,|\\.)[0-9]+)?((\\s)?per(\\s)?cento|(\\s)?%)',r'\\1_percento',str(x))))\n",
    "\n",
    "#removing contracted articles and prepositions\n",
    "colsel_merged['body'] = colsel_merged['body'].apply(lambda x: str(re.sub(r'[a-zA-Z]+\\’',r' ',str(x))))\n",
    "colsel_merged['body'] = colsel_merged['body'].apply(lambda x: str(re.sub(r'[a-zA-Z]+\\'',r' ',str(x))))\n",
    "\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r'(il)?( )?sole( )?24( )?ore','sole24ore')\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace('(milion(i|e)|miliard(i|o)|euro)','')\n",
    "\n",
    "#remove useless endings\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r'\\(c.fe.\\)','')\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r'((\\()?( )?sole24ore (radiocor)?( plus)?( )?(\\))?.\\-.*?)\\-','')\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r'(continua (da )?pagina [0-9]+)© riproduzione riservata(continua (da )?pagina [0-9]+)', '')\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r'((© )?riproduzione riser(vata)?).*','')\n",
    "\n",
    "#format urls\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r'(www\\.|http\\:\\/\\/|https\\:\\/\\/|\\.com|\\.net|\\.org|\\.it|@[a-zA-Z]+)','')\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?','')  \n",
    "\n",
    "#format dots for pos_tagging, done in order to mess up with urls!\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r':',' : ')\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r'[a-zA-Z]+\\'',' ')\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r'.','. ')\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace(r'( )+',' ')\n",
    "\n",
    "#replace abbreviation\n",
    "colsel_merged['body'] = colsel_merged['body'].str.replace('p/e','price_earnings_ratio')\n",
    "\n",
    "#adjusted html tags cleaning! 2 is the magic number\n",
    "for _ in np.arange(2):\n",
    "    colsel_merged['body'] = colsel_merged['body'].apply(lambda x: str(BeautifulSoup(x,\"lxml\").text))\n",
    "    \n",
    "#escaping &\n",
    "colsel_merged['body'] = colsel_merged['body'].apply(lambda x: str(re.sub(r'( )?\\&( )?','and',str(x))))\n",
    "\n",
    "\n",
    "#removing unprintable unicode\n",
    "colsel_merged['body'].replace('\\u200e','',regex=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'pietro scott jovane lascia la poltrona di amministratore delegato di rcsmedia group. al termine di una giornata fitta di incontri e partita in prima mattinata con il summit con le banche creditrici per la rinegoziazione del debito, il consiglio di amministrazione del gruppo editoriale ha comunicato uscita «consensuale» di scott jovane a partire dal prossimo 15 ottobre e assegnazione delle deleghe di jovane ad interim al presidente maurizio costa. «il cda di rcs mediagroup - si legge in una nota - ha preso atto della volontà ad pietro scott jovane di lasciare il proprio incarico, ritenendo concluso un ciclo della vita aziendale e manifestando intenzione di intraprendere un nuovo percorso professionale».  inoltre il consiglio ha comunicato che per la risoluzione del contratto saranno corrisposte 150 mila al manager, a cui si sommano 600 mila lordi per il patto di non concorrenza. il board, iniziato alle 17. 30 in via rizzoli, nella sede centrale di rcs mediagroup, dopo la riunione del comitato remunerazioni e del comitato nomine, è durato poco più di due ore. una riunione tranquilla, si apprende, che ha esaminato anche il tema della rinegoziazione dei termini del finanziamento con le banche creditrici. sulla questione in mattinata il cfo riccardo taranto aveva tenuto un primo incontro con i rappresentanti degli istituti di credito. nel corso incontro sarebbe stato chiesto alle banche, secondo indiscrezioni, di considerare la vendita di rcs libri come una dismissione riferita esercizio in corso, nonostante incasso effettivo il prossimo anno. uscita di jovane, che secondo indiscrezioni potrebbe avere un incarico in banzai, la società fondata da paolo ainio, suo grande amico, arriva dopo un mandato lungo tre anni che ha visto il manager impegnato in un complesso piano di risanamento per il gruppo. stretto tra le banche e gli azionisti, scott jovane ha portato a termine una serie di cessioni, ultima la vendita della divisione libri alla mondadori. origine della svolta, tuttavia, ci sarebbe stata insoddisfazione del consiglio di amministrazione per la gestione del manager e in particolare per i piani presentati per la ristrutturazione societaria, giudicati non adeguati dal board e insufficienti per traghettare rcs mediagroup fuori dalla difficile situazione finanziaria. da quanto si è appreso tra il manager e il consiglio ci sarebbe stata una progressiva incomprensione nel corso degli ultimi mesi, fino a una decisione che si vuole ora essere «consensuale». il passo indietro del manager apre così la questione della successione. girano diversi nomi, tra cui quello attuale direttore generale alessandro bompieri se dovesse prevalere la scelta di una candidatura interna. nella rosa dei nomi figurerebbero anche gli attuali consiglieri laura cioli e teresa cremisi. una scelta che dovrebbe avvenire in tempi stretti, ma che si preannuncia complessa. intanto per il compito che sarà affidato al nuovo amministratore delegato del gruppo, assai delicato. nonostante il successo della lunga e complessa trattativa sui libri,infatti, in rcs rispetto a quanto previsto dagli accordi con le banche creditrici alcune cose hanno preso una piega ben diversa. il gruppo di via rizzoli avrebbe dovuto realizzare entro fine settembre cessioni di attività non strategiche per 250 di e ne ha invece realizzate per circa 190 , libri esclusi. rcs sarebbe poi in trattativa con un primario gruppo internazionale per cedere le attività televisive di veo tv, il mux spagnolo, da cui potrebbero entrare altri 25 . le intese con le banche prevedevano che il debito netto a fine anno scendesse sotto i 440 e fosse inferiore a 3,5 volte il margine operativo lordo. già a fine settembre, poi, indebitamento netto si voleva non dovesse superare le 4,5 volte il margine operativo lordo. sfondando i vincoli con le banche (covenant) rcs era impegnata a varare subito una seconda trance di aumento di capitale, per circa 190 , sulla delega originaria da 600 avuta a suo tempo dai soci e già esercitata per oltre 400 . a fine giugno la semestrale rcs segnava una posizione finanziaria netta negativa per 526 . la priorità, dunque, è rivedere in tempi stretti gli accordi siglati con gli istituti. ma soprattutto valutare se quella delega sulla ricapitalizzazione sia ora necessaria. una eventualità che potrebbe ridisegnare gli assetti societari del gruppo già delicati. fino a che punto è ancora tutto da verificare. la borsa, sullo sfondo, teme questa nuova fase di incertezza. le azioni rcs hanno archiviato la seduta in forte calo del 6_percento a 0,92 dopo essere arrivata nel corso delle contrattatzioni a perdere 8_percento. ']\n"
     ]
    }
   ],
   "source": [
    "tmp= colsel_merged[colsel_merged['identificativo']=='FM20151009036ACmwyfCB']['body'].values\n",
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'la saga continua. nonostante che tutti sappiano come va a finire. ma il commissario peo che si occupa di policy digitale, günther oettinger, non ne è al corrente, evidentemente. la saga è quella degli editori che si sentono defraudati di un diritto a causa dei motori di ricerca - cioè di google - che propongono ai lettori qualche riga dei loro articoli insieme ai link per trovarli online. oettinger sta studiando una proposta di direttiva che dà agli editori il diritto di chiedere un compenso per quelle righe di testo. forse pensa così di apparire come eroe della saga, innovatore che risolve un problema a lungo dibattuto. invece, arriva ultimo. e dopo che questo genere di soluzione è già fallito in spagna e in germania. ma è veramente possibile che non lo sappia? e il vice presidente della commissione, andrus ansip, che conosce il digitale molto meglio di oettinger e che si occupa del mercato unico digitale, non gli ha detto nulla? in realtà, la realtà digitale va troppo veloce per i tempi della politica ed è troppo complessa per le sue banalizzazioni. e questo è un problema serio. di luca de biase in effetti, google governa una gran quantità di traffico online e il pubblico che clicca sui suoi link moltiplica le pagine viste sui giornali di tutti gli editori, che così aumentano le entrate pubblicitarie. se fossero solo i link, forse, non ci sarebbe molto da discutere. ma il motore di ricerca aggiunge ai link anche un paio di righe di testo tratte dagli articoli dei giornali stessi per far capire meglio ai lettori di che cosa si parla : si chiamano snippets. e gli editori hanno pensato per qualche tempo di chiedere a google un pagamento per quella, seppur minima, copiatura. «è pacifico che quelle poche righe non sono soggette al diritto autore» dice guido scorza, avvocato esperto di copyright : «ed è per questo che si studia una proposta di direttiva che introduca un diritto connesso al copyright, quello appunto di usare gli snippets». una decisione di questo tipo potrebbe aiutare gli editori? una cosa del genere è stata già tentata, in modi diversi, in spagna e in germania. in spagna, è stato introdotto obbligo per gli editori di richiedere il pagamento del diritto a pubblicare gli snippets. come conseguenza google ha smesso di pubblicarli e ha chiuso il servizio google news : questo ha fatto perdere agli editori spagnoli enorme quantità di traffico e di denaro. altra parte, google news non raccoglie pubblicità : è solo un servizio per il pubblico e per google chiuderlo non è un gran danno. in germania, invece, la legge ha introdotto la semplice possibilità per gli editori di far pagare gli snippet. e gli editori hanno scelto di esentare google dal pagamento per non perdere, appunto, traffico e denaro. stando alle indiscrezioni pubblicate dal financial times, la commissione potrebbe introdurre il diritto connesso per gli snippets in tutta pa, ma senza obbligare gli editori a esigerlo e senza obbligare google - ci mancherebbe altro - a pubblicare i link con le righe di testo esplicativo di cui si parla : la nuova regola, dunque, potrebbe avere un effetto nella teoria del diritto autore, ma in pratica non sposterebbe un solo . il problema degli editori, con ogni evidenza, non è tanto il copyright, quanto la tecnologia che serve per difenderlo e valorizzarlo nel mondo digitale. la stampa era un territorio molto più lento e controllato. il digitale va veloce, è complesso, richiede capacità e investimenti tecnologici che gli editori non hanno dimostrato di voler sviluppare, per anni guardando con occhio distratto a quello che facevano i pionieri del web, per poi trovarsi a fare le spese della propria arretratezza. ma ormai gli editori lo sanno. anche se a bruxelles non tutti se ne sono accorti. è molto più importante investimento di google innovazione degli editori della digital news initiative : con questa anche sole24ore, insieme a iit, expert system e fondazione golinelli, finanzierà la ricerca per realizzare un nuovo servizio editoriale per connettere gli scienziati e gli imprenditori. sulle strade del digitale non si guida guardando soltanto nello specchietto retrovisore. e davanti è il problema della qualità informazione, che google - e facebook, o twitter - non hanno certo risolto. per i giornali che guardano avanti, trovare il modo di sostenersi e crescere offrendo informazione di qualità con media adatti al mondo contemporaneo è la strada maestra. ed è possibile. tutto il resto è secondario. ']\n"
     ]
    }
   ],
   "source": [
    "print(colsel_merged[colsel_merged['identificativo']=='FM20160827022ADDGKkAB']['body'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_tag(tt):\n",
    "    if(len(tt)<3):\n",
    "        print(tt[0])\n",
    "    if(tt[2]=='@card@' or tt[2]=='@ord@'):\n",
    "        return tt[0]\n",
    "    else:\n",
    "        return re.sub(r'[a-z]+\\|','',tt[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_auxiliary_verbs(current, succ, flag):\n",
    "    if flag==0:\n",
    "        if current[2]=='essere' and succ[1].startswith('VER:'):\n",
    "            temp = clean_tag(current) + \"_\" + clean_tag(succ)\n",
    "            flag = 1\n",
    "            #print(temp)\n",
    "            return temp, flag\n",
    "        return clean_tag(current), flag\n",
    "    else:\n",
    "        flag = 0\n",
    "        return clean_tag(succ), flag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "li = [0, 1, 2, 3]\n",
    "\n",
    "\n",
    "for idx, elem in enumerate(li):\n",
    "    thiselem = elem\n",
    "    nextelem = li[(idx + 1) % len(li)]\n",
    "    print(str(thiselem) + \"\\t\" + str(nextelem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pos_tagger_ita(lst):\n",
    "    corpus = {}\n",
    "    stopw = set(stopwords.words(\"italian\"))\n",
    "    puntk = set(string.punctuation)\n",
    "    rem = stopw | puntk | set('«') | set('»')\n",
    "    tagger = treetaggerwrapper.TreeTagger(TAGLANG='it', TAGDIR=treetaggerPath)\n",
    "    for txt in lst:\n",
    "        tagged_txt = []\n",
    "        tags=tagger.tag_text(txt[1])\n",
    "        pp_tags= treetaggerwrapper.make_tags(tags)\n",
    "        #tagged_txt = [clean_tag(t) for idx,t in enumerate(pp_tags) if clean_tag(t) not in rem]\n",
    "        flag = 0\n",
    "        for idx,t in enumerate(pp_tags):\n",
    "            if idx != len(pp_tags):\n",
    "                t_succ = pp_tags[(idx + 1) % len(pp_tags)]\n",
    "                #print(str(t))\n",
    "                #print(clean_tag(t))\n",
    "                #print(\"####\")\n",
    "                \n",
    "                txt_tmp, flag = merge_auxiliary_verbs(t, t_succ, flag)\n",
    "                print(txt_tmp)\n",
    "                if (txt_tmp not in rem):\n",
    "                    tagged_txt.append(txt_tmp)\n",
    "        corpus[txt[0]]= tagged_txt\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t1= time.time()\n",
    "list_corp = colsel_merged[['identificativo','body']].values.tolist()\n",
    "corp = pos_tagger_ita(list_corp)\n",
    "print('Lemmatisation done in '+ str((time.time()-t1)/60) + ' min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['FM20160429032ACC9MUHD', 'acea è pronta a giocare la sua partita sulla banda ultralarga. «stiamo parlando con enel, telecom e governo» rispetto alla possibilità di partecipare alla posa della fibra ottica nella capitale. la conferma è arrivata ieri ad di acea, alberto irace, a margine assemblea dei soci che ha approvato il bilancio 2015. «abbiamo contatti - ha aggiunto - con quelli che se ne stanno occupando» e «stiamo valutando tutte le sinergie possibili, anche se questa prospettiva non è nel nostro piano industriale». il top manager ha poi auspicato un aumento, dal 20_percento al 40_percento, del flottante in borsa. «sarebbe importante per attrarre i fondi e i grandi investitori». ']]\n",
      "------\n",
      "a\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3b40a8433191>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_corp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tagger_ita\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_corp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-27-3e65fd7fa971>\u001b[0m in \u001b[0;36mpos_tagger_ita\u001b[0;34m(lst)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mtxt_tmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_auxiliary_verbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_succ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclean_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_tmp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                     \u001b[0mtagged_txt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtagged_txt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1129d23c2a05>\u001b[0m in \u001b[0;36mclean_tag\u001b[0;34m(tt)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'@card@'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'@ord@'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "test_corp = colsel_merged[['identificativo','body']].head(1).values.tolist()\n",
    "print(test_corp)\n",
    "print(\"------\")\n",
    "print(pos_tagger_ita(test_corp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'abbia',\n",
       " 'abbiamo',\n",
       " 'abbiano',\n",
       " 'abbiate',\n",
       " 'ad',\n",
       " 'agl',\n",
       " 'agli',\n",
       " 'ai',\n",
       " 'al',\n",
       " 'all',\n",
       " 'alla',\n",
       " 'alle',\n",
       " 'allo',\n",
       " 'anche',\n",
       " 'avemmo',\n",
       " 'avendo',\n",
       " 'avesse',\n",
       " 'avessero',\n",
       " 'avessi',\n",
       " 'avessimo',\n",
       " 'aveste',\n",
       " 'avesti',\n",
       " 'avete',\n",
       " 'aveva',\n",
       " 'avevamo',\n",
       " 'avevano',\n",
       " 'avevate',\n",
       " 'avevi',\n",
       " 'avevo',\n",
       " 'avrai',\n",
       " 'avranno',\n",
       " 'avrebbe',\n",
       " 'avrebbero',\n",
       " 'avrei',\n",
       " 'avremmo',\n",
       " 'avremo',\n",
       " 'avreste',\n",
       " 'avresti',\n",
       " 'avrete',\n",
       " 'avrà',\n",
       " 'avrò',\n",
       " 'avuta',\n",
       " 'avute',\n",
       " 'avuti',\n",
       " 'avuto',\n",
       " 'c',\n",
       " 'che',\n",
       " 'chi',\n",
       " 'ci',\n",
       " 'coi',\n",
       " 'col',\n",
       " 'come',\n",
       " 'con',\n",
       " 'contro',\n",
       " 'cui',\n",
       " 'da',\n",
       " 'dagl',\n",
       " 'dagli',\n",
       " 'dai',\n",
       " 'dal',\n",
       " 'dall',\n",
       " 'dalla',\n",
       " 'dalle',\n",
       " 'dallo',\n",
       " 'degl',\n",
       " 'degli',\n",
       " 'dei',\n",
       " 'del',\n",
       " 'dell',\n",
       " 'della',\n",
       " 'delle',\n",
       " 'dello',\n",
       " 'di',\n",
       " 'dov',\n",
       " 'dove',\n",
       " 'e',\n",
       " 'ebbe',\n",
       " 'ebbero',\n",
       " 'ebbi',\n",
       " 'ed',\n",
       " 'era',\n",
       " 'erano',\n",
       " 'eravamo',\n",
       " 'eravate',\n",
       " 'eri',\n",
       " 'ero',\n",
       " 'essendo',\n",
       " 'faccia',\n",
       " 'facciamo',\n",
       " 'facciano',\n",
       " 'facciate',\n",
       " 'faccio',\n",
       " 'facemmo',\n",
       " 'facendo',\n",
       " 'facesse',\n",
       " 'facessero',\n",
       " 'facessi',\n",
       " 'facessimo',\n",
       " 'faceste',\n",
       " 'facesti',\n",
       " 'faceva',\n",
       " 'facevamo',\n",
       " 'facevano',\n",
       " 'facevate',\n",
       " 'facevi',\n",
       " 'facevo',\n",
       " 'fai',\n",
       " 'fanno',\n",
       " 'farai',\n",
       " 'faranno',\n",
       " 'farebbe',\n",
       " 'farebbero',\n",
       " 'farei',\n",
       " 'faremmo',\n",
       " 'faremo',\n",
       " 'fareste',\n",
       " 'faresti',\n",
       " 'farete',\n",
       " 'farà',\n",
       " 'farò',\n",
       " 'fece',\n",
       " 'fecero',\n",
       " 'feci',\n",
       " 'fosse',\n",
       " 'fossero',\n",
       " 'fossi',\n",
       " 'fossimo',\n",
       " 'foste',\n",
       " 'fosti',\n",
       " 'fu',\n",
       " 'fui',\n",
       " 'fummo',\n",
       " 'furono',\n",
       " 'gli',\n",
       " 'ha',\n",
       " 'hai',\n",
       " 'hanno',\n",
       " 'ho',\n",
       " 'i',\n",
       " 'il',\n",
       " 'in',\n",
       " 'io',\n",
       " 'l',\n",
       " 'la',\n",
       " 'le',\n",
       " 'lei',\n",
       " 'li',\n",
       " 'lo',\n",
       " 'loro',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mi',\n",
       " 'mia',\n",
       " 'mie',\n",
       " 'miei',\n",
       " 'mio',\n",
       " 'ne',\n",
       " 'negl',\n",
       " 'negli',\n",
       " 'nei',\n",
       " 'nel',\n",
       " 'nell',\n",
       " 'nella',\n",
       " 'nelle',\n",
       " 'nello',\n",
       " 'noi',\n",
       " 'non',\n",
       " 'nostra',\n",
       " 'nostre',\n",
       " 'nostri',\n",
       " 'nostro',\n",
       " 'o',\n",
       " 'per',\n",
       " 'perché',\n",
       " 'più',\n",
       " 'quale',\n",
       " 'quanta',\n",
       " 'quante',\n",
       " 'quanti',\n",
       " 'quanto',\n",
       " 'quella',\n",
       " 'quelle',\n",
       " 'quelli',\n",
       " 'quello',\n",
       " 'questa',\n",
       " 'queste',\n",
       " 'questi',\n",
       " 'questo',\n",
       " 'sarai',\n",
       " 'saranno',\n",
       " 'sarebbe',\n",
       " 'sarebbero',\n",
       " 'sarei',\n",
       " 'saremmo',\n",
       " 'saremo',\n",
       " 'sareste',\n",
       " 'saresti',\n",
       " 'sarete',\n",
       " 'sarà',\n",
       " 'sarò',\n",
       " 'se',\n",
       " 'sei',\n",
       " 'si',\n",
       " 'sia',\n",
       " 'siamo',\n",
       " 'siano',\n",
       " 'siate',\n",
       " 'siete',\n",
       " 'sono',\n",
       " 'sta',\n",
       " 'stai',\n",
       " 'stando',\n",
       " 'stanno',\n",
       " 'starai',\n",
       " 'staranno',\n",
       " 'starebbe',\n",
       " 'starebbero',\n",
       " 'starei',\n",
       " 'staremmo',\n",
       " 'staremo',\n",
       " 'stareste',\n",
       " 'staresti',\n",
       " 'starete',\n",
       " 'starà',\n",
       " 'starò',\n",
       " 'stava',\n",
       " 'stavamo',\n",
       " 'stavano',\n",
       " 'stavate',\n",
       " 'stavi',\n",
       " 'stavo',\n",
       " 'stemmo',\n",
       " 'stesse',\n",
       " 'stessero',\n",
       " 'stessi',\n",
       " 'stessimo',\n",
       " 'steste',\n",
       " 'stesti',\n",
       " 'stette',\n",
       " 'stettero',\n",
       " 'stetti',\n",
       " 'stia',\n",
       " 'stiamo',\n",
       " 'stiano',\n",
       " 'stiate',\n",
       " 'sto',\n",
       " 'su',\n",
       " 'sua',\n",
       " 'sue',\n",
       " 'sugl',\n",
       " 'sugli',\n",
       " 'sui',\n",
       " 'sul',\n",
       " 'sull',\n",
       " 'sulla',\n",
       " 'sulle',\n",
       " 'sullo',\n",
       " 'suo',\n",
       " 'suoi',\n",
       " 'ti',\n",
       " 'tra',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tue',\n",
       " 'tuo',\n",
       " 'tuoi',\n",
       " 'tutti',\n",
       " 'tutto',\n",
       " 'un',\n",
       " 'una',\n",
       " 'uno',\n",
       " 'vi',\n",
       " 'voi',\n",
       " 'vostra',\n",
       " 'vostre',\n",
       " 'vostri',\n",
       " 'vostro',\n",
       " 'è'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopw = set(stopwords.words(\"italian\"))\n",
    "stopw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corp_list = list(corp.values())\n",
    "phrases = Phrases(corp_list)\n",
    "bigram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['investire',\n",
       " 'bb',\n",
       " 'essere',\n",
       " 'parte',\n",
       " 'richiedere',\n",
       " 'seppure',\n",
       " 'diverso',\n",
       " 'società',\n",
       " 'essere',\n",
       " 'ormai',\n",
       " 'vista',\n",
       " 'andare',\n",
       " 'lungo',\n",
       " 'essere_concretizzare',\n",
       " 'biotechnology',\n",
       " 'biotechnology',\n",
       " 'essere_quotare',\n",
       " 'parte',\n",
       " 'ultimo',\n",
       " '10',\n",
       " 'avere',\n",
       " 'senza',\n",
       " 'stesso',\n",
       " 'essere_salire',\n",
       " 'andamento',\n",
       " 'nbi',\n",
       " 'wall',\n",
       " 'tradizionale',\n",
       " 'ciò',\n",
       " 'biotecnologia',\n",
       " 'avere',\n",
       " 'forte',\n",
       " 'inizio',\n",
       " 'nasdaq',\n",
       " 'biotecnology',\n",
       " 'index',\n",
       " 'quasi',\n",
       " 'ultimo',\n",
       " '5',\n",
       " 'essere_invertire',\n",
       " 'andare',\n",
       " 'andare',\n",
       " 'cosa',\n",
       " 'essere_accadere',\n",
       " 'biotech',\n",
       " '20',\n",
       " 'luglio',\n",
       " 'avere',\n",
       " '4194',\n",
       " 'due',\n",
       " 'mese',\n",
       " '28',\n",
       " 'stesso',\n",
       " 'indice',\n",
       " 'viaggiare',\n",
       " 'quotare',\n",
       " '2989.',\n",
       " 'potere',\n",
       " 'essere_riprendere',\n",
       " 'danza',\n",
       " 'area',\n",
       " '3410.',\n",
       " 'tonfo',\n",
       " 'essere_stare',\n",
       " 'periodo',\n",
       " 'prospettiva',\n",
       " 'diverso',\n",
       " 'essere_stare',\n",
       " 'soprattutto',\n",
       " 'rallentamento',\n",
       " 'economia',\n",
       " 'variabile',\n",
       " 'avere',\n",
       " 'grande',\n",
       " 'avere',\n",
       " 'candidato',\n",
       " 'casa',\n",
       " 'avere',\n",
       " 'ben',\n",
       " 'solo',\n",
       " 'alcune',\n",
       " 'azienda',\n",
       " 'essere_stare',\n",
       " 'sollecitare',\n",
       " 'aggressivo',\n",
       " 'essere',\n",
       " 'essere_trattare',\n",
       " 'eccessivo',\n",
       " 'stesso',\n",
       " 'bb',\n",
       " 'avere',\n",
       " 'fine',\n",
       " 'circa',\n",
       " 'franco',\n",
       " 'ultimo',\n",
       " 'avere',\n",
       " 'ulteriormente',\n",
       " 'aumentare',\n",
       " 'esposizione',\n",
       " 'situazione',\n",
       " 'potere',\n",
       " 'però',\n",
       " 'terminale',\n",
       " 'utile',\n",
       " 'nasdaq',\n",
       " 'biotech',\n",
       " 'index',\n",
       " '91.',\n",
       " 'multiplo',\n",
       " 'essere',\n",
       " 'price_earnings_ratio',\n",
       " 'ben',\n",
       " 'wall',\n",
       " 'qui',\n",
       " 'shiller',\n",
       " 'sandp',\n",
       " '500',\n",
       " 'essere',\n",
       " 'media',\n",
       " 'circa',\n",
       " '14.',\n",
       " 'essere_finire',\n",
       " 'titolo',\n",
       " 'titolo',\n",
       " 'settore',\n",
       " 'essere_sopravvalutare',\n",
       " 'biotech',\n",
       " 'fondo',\n",
       " 'piazza',\n",
       " 'farmaco',\n",
       " 'essere_attendere',\n",
       " 'azionario',\n",
       " 'pausa',\n",
       " 'lunedì',\n",
       " 'avere',\n",
       " 'circa',\n",
       " 'meno',\n",
       " 'rapporto',\n",
       " 'price_earnings_ratio',\n",
       " 'andare',\n",
       " 'maniera',\n",
       " 'avere',\n",
       " 'poco',\n",
       " 'multiplo',\n",
       " 'break',\n",
       " 'giocoforza',\n",
       " 'guardare',\n",
       " 'large',\n",
       " 'utile',\n",
       " 'circa',\n",
       " '15.',\n",
       " 'sandp',\n",
       " '500.',\n",
       " 'approccio',\n",
       " 'mercato',\n",
       " 'fare',\n",
       " 'sembrare',\n",
       " 'obiezione',\n",
       " 'essere_sbagliare',\n",
       " 'bb',\n",
       " 'bb',\n",
       " 'grande',\n",
       " 'recente',\n",
       " 'avere',\n",
       " 'mid',\n",
       " 'cap',\n",
       " 'essere_crollare',\n",
       " 'molto',\n",
       " 'operatore',\n",
       " 'essere',\n",
       " 'diverso',\n",
       " 'ciò',\n",
       " 'bb',\n",
       " 'biotech',\n",
       " 'fermo',\n",
       " 'persistente',\n",
       " 'causa',\n",
       " 'avvicinare',\n",
       " 'elezione',\n",
       " 'presidenziale',\n",
       " 'prospettiva',\n",
       " 'ultimo',\n",
       " 'parte',\n",
       " 'fin',\n",
       " 'qui',\n",
       " 'alcune',\n",
       " 'indicazione',\n",
       " 'dinamica',\n",
       " 'bb',\n",
       " 'primis',\n",
       " 'andare',\n",
       " 'corso',\n",
       " 'terzo',\n",
       " 'portafoglio',\n",
       " 'avere',\n",
       " 'perdita',\n",
       " 'franco',\n",
       " 'primo',\n",
       " 'nove',\n",
       " 'mese',\n",
       " 'rendimento',\n",
       " 'totale',\n",
       " 'avere',\n",
       " 'rimanere',\n",
       " 'franco',\n",
       " 'intero',\n",
       " 'bb',\n",
       " 'biotech',\n",
       " 'rispondere',\n",
       " 'newsflow',\n",
       " 'attendere',\n",
       " 'nuovo',\n",
       " 'seppure',\n",
       " 'essere',\n",
       " 'evento',\n",
       " 'profitto',\n",
       " 'risultare',\n",
       " 'record',\n",
       " 'fine',\n",
       " 'dinamica',\n",
       " 'inevitabilmente',\n",
       " 'strategia',\n",
       " 'principale',\n",
       " 'bb',\n",
       " 'scorso',\n",
       " 'anno',\n",
       " 'notare',\n",
       " 'diverso',\n",
       " 'focus',\n",
       " 'restare',\n",
       " '2015',\n",
       " 'primo',\n",
       " 'nove',\n",
       " '2014.',\n",
       " 'discorso',\n",
       " 'malattia',\n",
       " 'peso',\n",
       " 'essere_passare',\n",
       " 'cardiovascolare',\n",
       " 'disturbo',\n",
       " 'avere',\n",
       " 'vedere',\n",
       " 'proprio',\n",
       " '9_percento',\n",
       " 'dinamico',\n",
       " 'nuovo',\n",
       " 'altro',\n",
       " 'bb',\n",
       " 'biotech',\n",
       " 'avere',\n",
       " 'sistema',\n",
       " 'contesto',\n",
       " 'bb',\n",
       " 'biotech',\n",
       " 'avere',\n",
       " 'alder',\n",
       " 'break',\n",
       " 'appunto',\n",
       " 'emicrania',\n",
       " 'essere_essere',\n",
       " 'solo',\n",
       " 'altro',\n",
       " 'frontiera',\n",
       " 'vedere',\n",
       " 'coinvolgere',\n",
       " 'juno',\n",
       " '1_percento',\n",
       " 'asset',\n",
       " 'kite',\n",
       " 'società',\n",
       " 'essere',\n",
       " 'attivo',\n",
       " 'sviluppare',\n",
       " 'nuovo',\n",
       " 'cellula',\n",
       " 'cellula',\n",
       " 'prospettiva',\n",
       " 'essere',\n",
       " 'sistema',\n",
       " 'potere',\n",
       " 'estendere',\n",
       " 'fin',\n",
       " 'qua',\n",
       " 'alcune',\n",
       " 'settore',\n",
       " 'altro',\n",
       " 'net',\n",
       " 'asset',\n",
       " 'scontare',\n",
       " 'essere_passare',\n",
       " 'marzo',\n",
       " 'fine',\n",
       " 'settembre',\n",
       " 'numero',\n",
       " 'valore',\n",
       " 'dare',\n",
       " 'tale',\n",
       " 'senso',\n",
       " 'bb',\n",
       " 'biotech',\n",
       " 'sottolineare',\n",
       " 'volere',\n",
       " 'fare',\n",
       " 'conoscere',\n",
       " 'proprio',\n",
       " 'essere_finalizzare',\n",
       " 'proprio',\n",
       " 'numero',\n",
       " 'lungo',\n",
       " 'ultimo',\n",
       " 'avere',\n",
       " 'senza',\n",
       " 'buy',\n",
       " 'suddetto',\n",
       " 'attività',\n",
       " 'rendere',\n",
       " 'remunerare',\n",
       " 'mercato',\n",
       " 'settore',\n",
       " 'quindi',\n",
       " 'titolo',\n",
       " 'cìò',\n",
       " 'prezzo',\n",
       " 'medio',\n",
       " 'interessare',\n",
       " 'soprattutto',\n",
       " 'implicitamente',\n",
       " 'indurre',\n",
       " 'grande',\n",
       " 'azione',\n",
       " 'andare',\n",
       " 'buy',\n",
       " 'back',\n",
       " 'capitale',\n",
       " '2013',\n",
       " 'essere_chiudere',\n",
       " 'assemblea',\n",
       " 'marzo',\n",
       " '2016.',\n",
       " 'esso',\n",
       " 'potere']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp['FM20151108016ACcDasOB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pietro',\n",
       " 'scott',\n",
       " 'jovane',\n",
       " 'amministratore',\n",
       " 'rcsmedia',\n",
       " 'giornata',\n",
       " 'primo',\n",
       " 'banca',\n",
       " 'gruppo',\n",
       " 'editoriale',\n",
       " 'avere',\n",
       " 'comunicare',\n",
       " 'scott',\n",
       " 'prossimo',\n",
       " '15',\n",
       " 'presidente',\n",
       " 'maurizio',\n",
       " 'rcs',\n",
       " 'avere',\n",
       " 'prendere',\n",
       " 'pietro',\n",
       " 'scott',\n",
       " 'proprio',\n",
       " 'ritenere',\n",
       " 'vita',\n",
       " 'manifestare',\n",
       " 'nuovo',\n",
       " 'percorso',\n",
       " 'consiglio',\n",
       " 'avere',\n",
       " 'contratto',\n",
       " 'essere_corrispondere',\n",
       " '150',\n",
       " '150',\n",
       " 'sommare',\n",
       " '600',\n",
       " 'mila',\n",
       " '17.',\n",
       " 'via',\n",
       " 'sede',\n",
       " 'rcs',\n",
       " 'comitato',\n",
       " 'comitato',\n",
       " 'essere_durare',\n",
       " 'poco',\n",
       " 'due',\n",
       " 'riunione',\n",
       " 'avere',\n",
       " 'banca',\n",
       " 'cfo',\n",
       " 'riccardo',\n",
       " 'taranto',\n",
       " 'avere',\n",
       " 'primo',\n",
       " 'corso',\n",
       " 'incontro',\n",
       " 'essere_stare',\n",
       " 'chiedere',\n",
       " 'secondo',\n",
       " 'rcs',\n",
       " 'dismissione',\n",
       " 'riferire',\n",
       " 'nonostante',\n",
       " 'incasso',\n",
       " 'prossimo',\n",
       " 'secondo',\n",
       " 'indiscrezione',\n",
       " 'potere',\n",
       " 'società',\n",
       " 'paolo',\n",
       " 'grande',\n",
       " 'arrivare',\n",
       " 'mandato',\n",
       " 'lungo',\n",
       " 'tre',\n",
       " 'avere',\n",
       " 'manager',\n",
       " 'complesso',\n",
       " 'scott',\n",
       " 'jovane',\n",
       " 'avere',\n",
       " 'divisione',\n",
       " 'essere_stare',\n",
       " 'insoddisfazione',\n",
       " 'piano',\n",
       " 'ristrutturazione',\n",
       " 'traghettare',\n",
       " 'rcs',\n",
       " 'mediagroup',\n",
       " 'difficile',\n",
       " 'situazione',\n",
       " 'essere_apprendere',\n",
       " 'stare',\n",
       " 'progressivo',\n",
       " 'ultimo',\n",
       " 'volere',\n",
       " 'ora',\n",
       " 'passo',\n",
       " 'manager',\n",
       " 'aprire',\n",
       " 'girare',\n",
       " 'diverso',\n",
       " 'attuale',\n",
       " 'direttore',\n",
       " 'generale',\n",
       " 'alessandro',\n",
       " 'dovere',\n",
       " 'candidatura',\n",
       " 'nome',\n",
       " 'attuale',\n",
       " 'consigliere',\n",
       " 'laura',\n",
       " 'teresa',\n",
       " 'dovere',\n",
       " 'tempo',\n",
       " 'preannunciare',\n",
       " 'essere_affidare',\n",
       " 'amministratore',\n",
       " 'amministratore',\n",
       " 'assai',\n",
       " 'complesso',\n",
       " 'rcs',\n",
       " 'banca',\n",
       " 'creditore',\n",
       " 'alcune',\n",
       " 'cosa',\n",
       " 'avere',\n",
       " 'piega',\n",
       " 'ben',\n",
       " 'via',\n",
       " 'rizzoli',\n",
       " 'avere',\n",
       " 'dovere',\n",
       " 'realizzare',\n",
       " 'entro',\n",
       " 'fine',\n",
       " 'settembre',\n",
       " 'avere',\n",
       " 'invece',\n",
       " 'circa',\n",
       " 'libro',\n",
       " 'rcs',\n",
       " 'essere',\n",
       " 'primario',\n",
       " 'gruppo',\n",
       " 'attività',\n",
       " 'veo',\n",
       " 'mux',\n",
       " 'potere',\n",
       " 'entrare',\n",
       " 'altro',\n",
       " 'banca',\n",
       " 'debito',\n",
       " 'fine',\n",
       " 'anno',\n",
       " 'scendere',\n",
       " 'essere',\n",
       " '5',\n",
       " 'margine',\n",
       " 'operativo',\n",
       " 'fine',\n",
       " 'indebitamento',\n",
       " 'dovere',\n",
       " '5',\n",
       " 'margine',\n",
       " 'operativo',\n",
       " 'rcs',\n",
       " 'essere_impegnare',\n",
       " 'subito',\n",
       " 'secondo',\n",
       " 'circa',\n",
       " 'delega',\n",
       " '600',\n",
       " 'già',\n",
       " 'oltre',\n",
       " 'fine',\n",
       " 'semestrale',\n",
       " 'rcs',\n",
       " 'posizione',\n",
       " 'finanziario',\n",
       " 'nettare',\n",
       " 'essere_rivedere',\n",
       " 'stretto',\n",
       " 'accordo',\n",
       " 'soprattutto',\n",
       " 'ricapitalizzazione',\n",
       " 'essere',\n",
       " 'ora',\n",
       " 'potere',\n",
       " 'assetto',\n",
       " 'gruppo',\n",
       " 'già',\n",
       " 'punto',\n",
       " 'essere',\n",
       " 'nuovo',\n",
       " 'azione',\n",
       " 'rcs',\n",
       " 'avere',\n",
       " 'forte',\n",
       " '92',\n",
       " 'dopo',\n",
       " 'essere_arrivare',\n",
       " '8_percento']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corp['FM20151009036ACmwyfCB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-12 14:32:49,156 : INFO : lang=it\n",
      "2017-04-12 14:32:49,158 : INFO : tagdir=/Users/albertomariopirovano/Documents/Programming/python/notebooks/tis_notebooks/treetagger/Darwin\n",
      "2017-04-12 14:32:49,159 : INFO : tagbin=/Users/albertomariopirovano/Documents/Programming/python/notebooks/tis_notebooks/treetagger/Darwin/bin/tree-tagger\n",
      "2017-04-12 14:32:49,161 : INFO : tagparfile=/Users/albertomariopirovano/Documents/Programming/python/notebooks/tis_notebooks/treetagger/Darwin/lib/italian-utf8.par\n",
      "2017-04-12 14:32:49,162 : INFO : tagopt=-token -lemma -sgml -quiet -no-unknown\n",
      "2017-04-12 14:32:49,164 : INFO : taginencoding=utf-8\n",
      "2017-04-12 14:32:49,165 : INFO : tagoutencoding=utf-8\n",
      "2017-04-12 14:32:49,166 : INFO : taginencerr=replace\n",
      "2017-04-12 14:32:49,167 : INFO : tagoutencerr=replace\n",
      "2017-04-12 14:32:49,169 : WARNING : Abbreviation file not found: italian-abbreviations-utf8\n",
      "2017-04-12 14:32:49,170 : WARNING : Processing without abbreviations file.\n",
      "2017-04-12 14:32:49,171 : INFO : abbrevfile=None\n",
      "2017-04-12 14:32:49,179 : INFO : Started TreeTagger from command: ['/Users/albertomariopirovano/Documents/Programming/python/notebooks/tis_notebooks/treetagger/Darwin/bin/tree-tagger', '-token', '-lemma', '-sgml', '-quiet', '-no-unknown', '/Users/albertomariopirovano/Documents/Programming/python/notebooks/tis_notebooks/treetagger/Darwin/lib/italian-utf8.par']\n",
      "2017-04-12 14:32:49,186 : INFO : Writing starting part to pipe.\n",
      "2017-04-12 14:32:49,189 : INFO : Writing data to pipe.\n",
      "2017-04-12 14:32:49,191 : INFO : Writing ending and flushing part to pipe.\n",
      "2017-04-12 14:32:49,192 : INFO : Finished writing data to pipe. Pipe flushed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Tag(word='quanto', pos='PRO:indef', lemma='quanto'),\n",
       " Tag(word='all’indebitamento', pos='NOM', lemma='all’indebitamento'),\n",
       " Tag(word=',', pos='PON', lemma=','),\n",
       " Tag(word='a', pos='PRE', lemma='a'),\n",
       " Tag(word='fine', pos='ADJ', lemma='fine'),\n",
       " Tag(word='settembre', pos='NOM', lemma='settembre'),\n",
       " Tag(word='l’asticella', pos='VER:remo', lemma='l’asticella'),\n",
       " Tag(word='della', pos='PRE:det', lemma='del'),\n",
       " Tag(word='società', pos='NOM', lemma='società'),\n",
       " Tag(word='guidata', pos='VER:pper', lemma='guidare'),\n",
       " Tag(word='da', pos='PRE', lemma='da'),\n",
       " Tag(word='stefano', pos='VER:pres', lemma='stefano'),\n",
       " Tag(word='ciccotti', pos='VER:impe', lemma='ciccotti'),\n",
       " Tag(word='e', pos='CON', lemma='e'),\n",
       " Tag(word='presieduta', pos='VER:pper', lemma='presiedere'),\n",
       " Tag(word='da', pos='PRE', lemma='da'),\n",
       " Tag(word='camillo', pos='NOM', lemma='camillo'),\n",
       " Tag(word='rossotto', pos='ADJ', lemma='rossotto'),\n",
       " Tag(word='si', pos='PRO:refl', lemma='si'),\n",
       " Tag(word='è', pos='VER:pres', lemma='essere'),\n",
       " Tag(word='fermata', pos='VER:pper', lemma='fermare'),\n",
       " Tag(word='a', pos='PRE', lemma='a'),\n",
       " Tag(word='49', pos='NUM', lemma='@card@'),\n",
       " Tag(word=',', pos='PON', lemma=','),\n",
       " Tag(word='6', pos='NUM', lemma='@card@'),\n",
       " Tag(word='milioni', pos='NOM', lemma='milione'),\n",
       " Tag(word='a', pos='PRE', lemma='a'),\n",
       " Tag(word='fronte', pos='NOM', lemma='fronte'),\n",
       " Tag(word='dei', pos='PRE:det', lemma='del'),\n",
       " Tag(word='65', pos='NUM', lemma='@card@'),\n",
       " Tag(word=',', pos='PON', lemma=','),\n",
       " Tag(word='5', pos='NUM', lemma='@card@'),\n",
       " Tag(word='milioni', pos='NOM', lemma='milione'),\n",
       " Tag(word='registrati', pos='VER:pper', lemma='registrare'),\n",
       " Tag(word='al', pos='PRE:det', lemma='al'),\n",
       " Tag(word='31', pos='NUM', lemma='@card@'),\n",
       " Tag(word='dicembre', pos='NOM', lemma='dicembre'),\n",
       " Tag(word='2014.', pos='ORD', lemma='@ord@')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='it', TAGDIR=treetaggerPath)\n",
    "tags=tagger.tag_text('quanto all’indebitamento, a fine settembre l’asticella della società guidata da stefano ciccotti e presieduta da camillo rossotto si è fermata a 49,6 milioni a fronte dei 65,5 milioni registrati al 31 dicembre 2014.')\n",
    "treetaggerwrapper.make_tags(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 20   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 20          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-12 14:33:07,956 : INFO : collecting all words and their counts\n",
      "2017-04-12 14:33:07,958 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-12 14:33:08,084 : INFO : collected 21228 word types from a corpus of 520868 raw words and 9283 sentences\n",
      "2017-04-12 14:33:08,086 : INFO : Loading a fresh vocabulary\n",
      "2017-04-12 14:33:08,110 : INFO : min_count=20 retains 2985 unique words (14% of original 21228, drops 18243)\n",
      "2017-04-12 14:33:08,112 : INFO : min_count=20 leaves 458896 word corpus (88% of original 520868, drops 61972)\n",
      "2017-04-12 14:33:08,131 : INFO : deleting the raw counts dictionary of 21228 items\n",
      "2017-04-12 14:33:08,134 : INFO : sample=0.001 downsamples 44 most-common words\n",
      "2017-04-12 14:33:08,135 : INFO : downsampling leaves estimated 394159 word corpus (85.9% of prior 458896)\n",
      "2017-04-12 14:33:08,137 : INFO : estimated required memory for 2985 words and 300 dimensions: 8656500 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-12 14:33:08,160 : INFO : resetting layer weights\n",
      "2017-04-12 14:33:08,261 : INFO : training model with 4 workers on 2985 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=20\n",
      "2017-04-12 14:33:08,263 : INFO : expecting 9283 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-04-12 14:33:09,279 : INFO : PROGRESS: at 18.52% examples, 372404 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-12 14:33:10,300 : INFO : PROGRESS: at 41.66% examples, 418569 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-12 14:33:11,308 : INFO : PROGRESS: at 64.38% examples, 442871 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-12 14:33:12,329 : INFO : PROGRESS: at 89.79% examples, 454073 words/s, in_qsize 7, out_qsize 0\n",
      "2017-04-12 14:33:12,536 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-04-12 14:33:12,545 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-12 14:33:12,549 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-12 14:33:12,553 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-12 14:33:12,554 : INFO : training on 2604340 raw words (1971178 effective words) took 4.3s, 459797 effective words/s\n",
      "2017-04-12 14:33:12,556 : INFO : precomputing L2-norms of word weight vectors\n",
      "2017-04-12 14:33:12,612 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2017-04-12 14:33:12,613 : INFO : not storing attribute syn0norm\n",
      "2017-04-12 14:33:12,613 : INFO : not storing attribute cum_table\n",
      "2017-04-12 14:33:13,320 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(list(corp.values()), workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emirato', 0.9694283604621887),\n",
       " ('consegna', 0.9631837606430054),\n",
       " ('aereo', 0.9556213617324829),\n",
       " ('catena', 0.9519364833831787),\n",
       " ('terminal', 0.9505414962768555),\n",
       " ('world', 0.9504255652427673),\n",
       " ('germania', 0.9502667188644409),\n",
       " ('vard', 0.9499741792678833),\n",
       " ('inaugurare', 0.9496723413467407),\n",
       " ('hmshost', 0.9462617635726929)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"los\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_points = 2000\n",
    "\n",
    "tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "two_d_embeddings = tsne.fit_transform(model.wv.syn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "for word, vocab_obj in model.wv.vocab.items():\n",
    "    d[word] = model.wv.vocab[word].count\n",
    "sorted_x = sorted(d.items(), key=operator.itemgetter(1))\n",
    "model.wv.vocab['moncler'].count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-d6897bafdf58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msorted_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#plot(two_d_embeddings, words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-d6897bafdf58>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpylab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msorted_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_points\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#plot(two_d_embeddings, words)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "def plot(embeddings, labels):\n",
    "    assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "    pylab.figure(figsize=(15,15))  # in inches\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = embeddings[i,:]\n",
    "        pylab.scatter(x, y)\n",
    "        pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                   ha='right', va='bottom')\n",
    "    pylab.show()\n",
    "\n",
    "words = [sorted_x[len(sorted_x)-i][0] for i in np.arange(0,num_points)]\n",
    "words\n",
    "#plot(two_d_embeddings, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.most_similar(\"biotecnologia\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
